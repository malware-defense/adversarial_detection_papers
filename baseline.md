- Detecting adversarial examples on deep neural networks with mutual information neural estimation (MIAED)
  - On detecting adversarial perturbations (GND)
  - Detecting adversarial samples from artifacts (KD+BU)
  - Characterizing adversarial subspaces using local intrinsic dimensionality (LID)
  - Adversarial and clean data are not twins (SSD)
  - Detecting adversarial image examples in deep neural networks with adaptive noise reduction (TSD)

- A Simple Unsupervised Data Depth-based Method to Detect Adversarial Images
  - Feature squeezing: Detecting adversarial examples in deep neural networks (FS)
  - Magnet: A two-pronged defense against adversarial examples (Magnet)

- EMShepherd: Detecting Adversarial Samples via Side-channel Leakage
  - Detecting adversarial samples with neural network invariant checking. (KDE)
  - Detecting adversarial samples from artifacts. (NIC)
  - Feature squeezing: Detecting adversarial examples in deep neural networks (FS)
  - Magnet: A two-pronged defense against adversarial examples (Magnet)
  
- Adversarial Detection by Latent Style Transformations [image transformation based multi-point defense techniques]
  - Defense-gan:Protecting classifiers against adversarial attacks using generative models ( Defense-GAN)
  - Magnet: a two-pronged defense against adversarial examples (Magnet)
  - Featurized bidirectional gan:Adversarial defense via adversarially learned semantic inference (FBGAN)
  - Detecting adversarial examples via neural fingerprinting (NFP)
  - Detecting adversarial examples through image transformation (RIT)
  
- ViDetecting adversarial examples is (nearly) as hard as classifying them 
  
- {WaveGuard}: Understanding and mitigating audio adversarial examples [audio domin, attack和defense均是选取本领域的，可借鉴]
  - Isolated and ensemble audio preprocessing methods for detecting adversarial examples against automatic speech recognition
  - Characterizing audio adversarial examples using temporal dependency
  
  - Magnet: a two-pronged defense against adversarial examples
  - Countering adversarial images using input transformations
  - Defensive quantization: When efficiency meets robustness
  - Qusecnets: Quantization based defense mechanism for securing deep neural network against adversarial attacks
  - Detecting adversarial image examples in deep neural networks with adaptive noise reduction


- Attack as Detection: Using Adversarial Attack Methods to Detect Abnormal Examples
  - Detecting adversarial samples from artifacts.
  - Characterizing Adversarial Subspaces Using Local Intrinsic Dimensionality
  - Dissector: Input Validation for Deep Learning Applications by Crossing-layer Dissection.
  - Adversarial sample detection for deep neural network through model mutation testing.


- Beating attackers at their own games: Adversarial example detection using adversarial gradient directions
  - A New Defense Against Adversarial Images: Turning a Weakness into a Strength.
  - Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. (FS)
  - A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks.
  - Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning. (DkNN)


- Adversarial Example Detection Using Latent Neighborhood Graph
  - Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning. (DkNN)
  - Defense against adversarial images using web-scale nearest-neighbor search. (KNN)
  - Characterizing adversarial subspaces using local intrinsic dimensionality (LID)
  - A new defense against adversarial images: Turning a weakness into a strength.

- Detecting Adversarial Examples from Sensitivity Inconsistency of Spatial-Transform Domain (schemes based)
  - Characterizing adversarial subspaces using local intrinsic dimensionality (LID)
  - A Simple Unified Framework for Detecting Out-Of-Distribution Samples and Adversarial Attacks. 
  - Feature Squeezing: Detecting Adversarial Examples in Deep Neural Networks. (FS)


- Class-disentanglement and applications in adversarial detection and defense (adversarial training based methods and preprocessing based methods)

- Detecting adversarial attacks via subset scanning of autoencoder activations and reconstruction error
  - Defense-gan: Protecting classifiers against adversarial attacks using generative models.
  - baseline
  

- Libre: A practical bayesian approach to adversarial detection
  - the fine-tuning start point MAP; 
  - two standard adversarial detection approaches KD [14] and LID [39], which both work on the extracted features by MAP; 
  - three popular BNN baselines MC dropout [17], MFVI [2], and LMFVI.